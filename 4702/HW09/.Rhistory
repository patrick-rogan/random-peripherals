rad + tax,
data = Boston, subset = train)
lda.pred = predict(lda.fit, Boston.test)
lda.class = lda.pred$class
table(lda.class, crim01.test)
mean(lda.class == crim01.test)
train.X = as.matrix(cbind(indus, nox, dis,
rad, tax)[train,])
test.X = as.matrix(cbind(indus, nox, dis,
rad, tax)[!train,])
train.crim01 = crim01[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.crim01, k = 1)
table(knn.pred, crim01.test)
glm.fit = glm(crim01 ~  nox + age + dis+
rad + tax,
family = binomial, data = Boston, subset = train)
glm.probs = predict(glm.fit, Boston.test, type = "response")
glm.pred = rep(0,length(crim01.test))
glm.pred[glm.probs > 0.5] = 1
table(glm.pred, crim01.test)
mean(glm.pred == crim01.test)
lda.fit = lda(crim01 ~  nox + age + dis+
rad + tax,
data = Boston, subset = train)
lda.pred = predict(lda.fit, Boston.test)
lda.class = lda.pred$class
table(lda.class, crim01.test)
mean(lda.class == crim01.test)
train.X = as.matrix(cbind( nox, age, dis,
rad, tax)[train,])
test.X = as.matrix(cbind( nox, age, dis,
rad, tax)[!train,])
train.crim01 = crim01[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.crim01, k = 1)
table(knn.pred, crim01.test)
glm.fit = glm(crim01 ~ zn + indus + chas + nox + rm + age + dis+
rad + tax + ptratio + black +lstat + medv,
family = binomial, data = Boston, subset = train)
glm.probs = predict(glm.fit, Boston.test, type = "response")
glm.pred = rep(0,length(crim01.test))
glm.pred[glm.probs > 0.5] = 1
table(glm.pred, crim01.test)
mean(glm.pred == crim01.test)
lda.fit = lda(crim01 ~ zn + indus + chas + nox + rm + age + dis+
rad + tax + ptratio + black +lstat + medv,
data = Boston, subset = train)
lda.pred = predict(lda.fit, Boston.test)
lda.class = lda.pred$class
table(lda.class, crim01.test)
mean(lda.class == crim01.test)
train.X = as.matrix(cbind(zn, indus, chas, nox, rm, age, dis,
rad, tax, ptratio, black, lstat, medv)[train,])
test.X = as.matrix(cbind(zn, indus, chas, nox, rm, age, dis,
rad, tax, ptratio, black, lstat, medv)[!train,])
train.crim01 = crim01[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.crim01, k = 1)
table(knn.pred, crim01.test)
# ii. First subset of predictors: indus, nox, age, dis, rad, tax, ptratio, and lstat
glm.fit = glm(crim01 ~ indus + nox + age + dis+
rad + tax + ptratio +lstat,
family = binomial, data = Boston, subset = train)
glm.probs = predict(glm.fit, Boston.test, type = "response")
glm.pred = rep(0,length(crim01.test))
glm.pred[glm.probs > 0.5] = 1
table(glm.pred, crim01.test)
mean(glm.pred == crim01.test)
lda.fit = lda(crim01 ~ indus + nox + age + dis+
rad + tax + ptratio +lstat,
data = Boston, subset = train)
lda.pred = predict(lda.fit, Boston.test)
lda.class = lda.pred$class
table(lda.class, crim01.test)
mean(lda.class == crim01.test)
train.X = as.matrix(cbind(indus, nox, age, dis,
rad, tax, ptratio, lstat)[train,])
test.X = as.matrix(cbind(indus, nox, age, dis,
rad, tax, ptratio, lstat)[!train,])
train.crim01 = crim01[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.crim01, k = 1)
table(knn.pred, crim01.test)
# iii. Second subset of predictors: indus, nox, age, dis, rad, tax
glm.fit = glm(crim01 ~ indus + nox + age + dis+
rad + tax,
family = binomial, data = Boston, subset = train)
glm.probs = predict(glm.fit, Boston.test, type = "response")
glm.pred = rep(0,length(crim01.test))
glm.pred[glm.probs > 0.5] = 1
table(glm.pred, crim01.test)
mean(glm.pred == crim01.test)
lda.fit = lda(crim01 ~ indus + nox + age + dis+
rad + tax,
data = Boston, subset = train)
lda.pred = predict(lda.fit, Boston.test)
lda.class = lda.pred$class
table(lda.class, crim01.test)
mean(lda.class == crim01.test)
train.X = as.matrix(cbind(indus, nox, age, dis,
rad, tax)[train,])
test.X = as.matrix(cbind(indus, nox, age, dis,
rad, tax)[!train,])
train.crim01 = crim01[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.crim01, k = 1)
table(knn.pred, crim01.test)
# iv. Several subsets of iii in search of the best result, use greedy approach
# remove predictors that improve the model starting from tax working up to indus
# note this was done with the randomized train indices. Turns out the removal of
# any one of these parameters does not improve all of the models.
glm.fit = glm(crim01 ~ indus + nox + age + dis+
rad + tax,
family = binomial, data = Boston, subset = train)
glm.probs = predict(glm.fit, Boston.test, type = "response")
glm.pred = rep(0,length(crim01.test))
glm.pred[glm.probs > 0.5] = 1
table(glm.pred, crim01.test)
mean(glm.pred == crim01.test)
lda.fit = lda(crim01 ~ indus + nox + age + dis+
rad + tax,
data = Boston, subset = train)
lda.pred = predict(lda.fit, Boston.test)
lda.class = lda.pred$class
table(lda.class, crim01.test)
mean(lda.class == crim01.test)
train.X = as.matrix(cbind(indus, nox, age, dis,
rad, tax)[train,])
test.X = as.matrix(cbind(indus, nox, age, dis,
rad, tax)[!train,])
train.crim01 = crim01[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.crim01, k = 1)
table(knn.pred, crim01.test)
train[320:406] = TRUE # second method, just look at last 86 points for test
Boston.test = Boston[!train,]
crim01.test = crim01[!train]
# i. All predictors
glm.fit = glm(crim01 ~ zn + indus + chas + nox + rm + age + dis+
rad + tax + ptratio + black +lstat + medv,
family = binomial, data = Boston, subset = train)
glm.probs = predict(glm.fit, Boston.test, type = "response")
glm.pred = rep(0,length(crim01.test))
glm.pred[glm.probs > 0.5] = 1
table(glm.pred, crim01.test)
mean(glm.pred == crim01.test)
lda.fit = lda(crim01 ~ zn + indus + chas + nox + rm + age + dis+
rad + tax + ptratio + black +lstat + medv,
data = Boston, subset = train)
lda.pred = predict(lda.fit, Boston.test)
lda.class = lda.pred$class
table(lda.class, crim01.test)
mean(lda.class == crim01.test)
train.X = as.matrix(cbind(zn, indus, chas, nox, rm, age, dis,
rad, tax, ptratio, black, lstat, medv)[train,])
test.X = as.matrix(cbind(zn, indus, chas, nox, rm, age, dis,
rad, tax, ptratio, black, lstat, medv)[!train,])
train.crim01 = crim01[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.crim01, k = 1)
table(knn.pred, crim01.test)
# ii. First subset of predictors: indus, nox, age, dis, rad, tax, ptratio, and lstat
glm.fit = glm(crim01 ~ indus + nox + age + dis+
rad + tax + ptratio +lstat,
family = binomial, data = Boston, subset = train)
glm.probs = predict(glm.fit, Boston.test, type = "response")
glm.pred = rep(0,length(crim01.test))
glm.pred[glm.probs > 0.5] = 1
table(glm.pred, crim01.test)
mean(glm.pred == crim01.test)
lda.fit = lda(crim01 ~ indus + nox + age + dis+
rad + tax + ptratio +lstat,
data = Boston, subset = train)
lda.pred = predict(lda.fit, Boston.test)
lda.class = lda.pred$class
table(lda.class, crim01.test)
mean(lda.class == crim01.test)
train.X = as.matrix(cbind(indus, nox, age, dis,
rad, tax, ptratio, lstat)[train,])
test.X = as.matrix(cbind(indus, nox, age, dis,
rad, tax, ptratio, lstat)[!train,])
train.crim01 = crim01[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.crim01, k = 1)
table(knn.pred, crim01.test)
# iii. Second subset of predictors: indus, nox, age, dis, rad, tax
glm.fit = glm(crim01 ~ indus + nox + age + dis+
rad + tax,
family = binomial, data = Boston, subset = train)
glm.probs = predict(glm.fit, Boston.test, type = "response")
glm.pred = rep(0,length(crim01.test))
glm.pred[glm.probs > 0.5] = 1
table(glm.pred, crim01.test)
mean(glm.pred == crim01.test)
lda.fit = lda(crim01 ~ indus + nox + age + dis+
rad + tax,
data = Boston, subset = train)
lda.pred = predict(lda.fit, Boston.test)
lda.class = lda.pred$class
table(lda.class, crim01.test)
mean(lda.class == crim01.test)
train.X = as.matrix(cbind(indus, nox, age, dis,
rad, tax)[train,])
test.X = as.matrix(cbind(indus, nox, age, dis,
rad, tax)[!train,])
train.crim01 = crim01[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.crim01, k = 1)
table(knn.pred, crim01.test)
# iv. Several subsets of iii in search of the best result, use greedy approach
# remove predictors that improve the model starting from tax working up to indus
# note this was done with the randomized train indices. Turns out the removal of
# any one of these parameters does not improve all of the models.
glm.fit = glm(crim01 ~ indus + nox + age + dis+
rad + tax,
family = binomial, data = Boston, subset = train)
glm.probs = predict(glm.fit, Boston.test, type = "response")
glm.pred = rep(0,length(crim01.test))
glm.pred[glm.probs > 0.5] = 1
table(glm.pred, crim01.test)
mean(glm.pred == crim01.test)
lda.fit = lda(crim01 ~ indus + nox + age + dis+
rad + tax,
train = rep(FALSE,length(crim01))
train[train.ix] = TRUE
#train[320:406] = TRUE # second method, just look at last 86 points for test
Boston.test = Boston[!train,]
train.ix = as.integer(createDataPartition(Boston$crim01, p = .8, list = FALSE, times = 1))
train = rep(FALSE,length(crim01))
train[train.ix] = TRUE
#train[320:406] = TRUE # second method, just look at last 86 points for test
Boston.test = Boston[!train,]
crim01.test = crim01[!train]
# i. All predictors
glm.fit = glm(crim01 ~ zn + indus + chas + nox + rm + age + dis+
rad + tax + ptratio + black +lstat + medv,
family = binomial, data = Boston, subset = train)
glm.probs = predict(glm.fit, Boston.test, type = "response")
glm.pred = rep(0,length(crim01.test))
glm.pred[glm.probs > 0.5] = 1
table(glm.pred, crim01.test)
mean(glm.pred == crim01.test)
lda.fit = lda(crim01 ~ zn + indus + chas + nox + rm + age + dis+
rad + tax + ptratio + black +lstat + medv,
data = Boston, subset = train)
lda.pred = predict(lda.fit, Boston.test)
lda.class = lda.pred$class
table(lda.class, crim01.test)
mean(lda.class == crim01.test)
train.X = as.matrix(cbind(zn, indus, chas, nox, rm, age, dis,
rad, tax, ptratio, black, lstat, medv)[train,])
test.X = as.matrix(cbind(zn, indus, chas, nox, rm, age, dis,
rad, tax, ptratio, black, lstat, medv)[!train,])
train.crim01 = crim01[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.crim01, k = 1)
table(knn.pred, crim01.test)
# ii. First subset of predictors: indus, nox, age, dis, rad, tax, ptratio, and lstat
glm.fit = glm(crim01 ~ indus + nox + age + dis+
rad + tax + ptratio +lstat,
family = binomial, data = Boston, subset = train)
glm.probs = predict(glm.fit, Boston.test, type = "response")
glm.pred = rep(0,length(crim01.test))
glm.pred[glm.probs > 0.5] = 1
table(glm.pred, crim01.test)
mean(glm.pred == crim01.test)
lda.fit = lda(crim01 ~ indus + nox + age + dis+
rad + tax + ptratio +lstat,
data = Boston, subset = train)
lda.pred = predict(lda.fit, Boston.test)
lda.class = lda.pred$class
table(lda.class, crim01.test)
mean(lda.class == crim01.test)
mean(lda.class == crim01.test)
train.X = as.matrix(cbind(indus, nox, age, dis,
rad, tax, ptratio, lstat)[train,])
test.X = as.matrix(cbind(indus, nox, age, dis,
rad, tax, ptratio, lstat)[!train,])
train.crim01 = crim01[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.crim01, k = 1)
table(knn.pred, crim01.test)
# iii. Second subset of predictors: indus, nox, age, dis, rad, tax
glm.fit = glm(crim01 ~ indus + nox + age + dis+
rad + tax,
family = binomial, data = Boston, subset = train)
glm.probs = predict(glm.fit, Boston.test, type = "response")
glm.pred = rep(0,length(crim01.test))
glm.pred[glm.probs > 0.5] = 1
table(glm.pred, crim01.test)
mean(glm.pred == crim01.test)
lda.fit = lda(crim01 ~ indus + nox + age + dis+
rad + tax,
data = Boston, subset = train)
lda.pred = predict(lda.fit, Boston.test)
lda.class = lda.pred$class
table(lda.class, crim01.test)
mean(lda.class == crim01.test)
train.X = as.matrix(cbind(indus, nox, age, dis,
rad, tax)[train,])
test.X = as.matrix(cbind(indus, nox, age, dis,
rad, tax)[!train,])
train.crim01 = crim01[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.crim01, k = 1)
table(knn.pred, crim01.test)
# iv. Several subsets of iii in search of the best result, use greedy approach
# remove predictors that improve the model starting from tax working up to indus
# note this was done with the randomized train indices. Turns out the removal of
# any one of these parameters does not improve all of the models.
glm.fit = glm(crim01 ~ indus + nox + age + dis+
rad + tax,
family = binomial, data = Boston, subset = train)
glm.probs = predict(glm.fit, Boston.test, type = "response")
glm.pred = rep(0,length(crim01.test))
glm.pred[glm.probs > 0.5] = 1
table(glm.pred, crim01.test)
mean(glm.pred == crim01.test)
lda.fit = lda(crim01 ~ indus + nox + age + dis+
rad + tax,
data = Boston, subset = train)
lda.pred = predict(lda.fit, Boston.test)
lda.class = lda.pred$class
table(lda.class, crim01.test)
mean(lda.class == crim01.test)
0 49  2
1  1 48
3/(3+49+48)
# 4.7.6 -------------------------------------------------------------------
beta0 = -6
beta1 = 0.05
beta2 = 1
x1=40
x2=3.5
exp(beta0+beta1*x1+beta2*x2)/(1+exp(beta0+beta1*x1+beta2*x2))
uniroot(function(x) exp(-6+0.05*x+1*3.5)/(1+exp(-6+0.05*x+1*3.5))-0.5, c(0,100))
x = 4
mu1 = 10 #average %profit for dividend issuing companies
mu2 = 0.0 #average %profit for non-dividend issuing companies
var_hat = 36
pi1 = 0.8 # %companies issuing dividends
pi2 = 0.2
pi1*exp(-1/2*1/var_hat*(x-mu1)^2)/
(pi1*exp(-1/2*1/var_hat*(x-mu1)^2) + pi2*exp(-1/2*1/var_hat*(x-mu2)^2))
.16/(1-.16)
library(MASS)
attach(Boston)
crim01 = rep(0,length(crim))
crim01[crim > median(crim)] = 1
Boston = data.frame(Boston,crim01)
set.seed(1) #Looking at crim01 there are obvious streaks of 0s and 1s, we sample randomly
# to reduce any impact on our final results
train.ix = as.integer(createDataPartition(Boston$crim01, p = .8, list = FALSE, times = 1))
library(caret) #use createDataPartition from caret librart to create randomized 80:20
set.seed(1) #Looking at crim01 there are obvious streaks of 0s and 1s, we sample randomly
# to reduce any impact on our final results
train.ix = as.integer(createDataPartition(Boston$crim01, p = .8, list = FALSE, times = 1))
train = rep(FALSE,length(crim01))
train[train.ix] = TRUE
#train[320:406] = TRUE # second method, just look at last 86 points for test
Boston.test = Boston[!train,]
crim01.test = crim01[!train]
# i. All predictors
glm.fit = glm(crim01 ~ zn + indus + chas + nox + rm + age + dis+
rad + tax + ptratio + black +lstat + medv,
family = binomial, data = Boston, subset = train)
glm.probs = predict(glm.fit, Boston.test, type = "response")
glm.pred = rep(0,length(crim01.test))
glm.pred[glm.probs > 0.5] = 1
table(glm.pred, crim01.test)
mean(glm.pred == crim01.test)
lda.fit = lda(crim01 ~ zn + indus + chas + nox + rm + age + dis+
rad + tax + ptratio + black +lstat + medv,
data = Boston, subset = train)
lda.pred = predict(lda.fit, Boston.test)
lda.class = lda.pred$class
table(lda.class, crim01.test)
mean(lda.class == crim01.test)
train.X = as.matrix(cbind(zn, indus, chas, nox, rm, age, dis,
rad, tax, ptratio, black, lstat, medv)[train,])
test.X = as.matrix(cbind(zn, indus, chas, nox, rm, age, dis,
rad, tax, ptratio, black, lstat, medv)[!train,])
train.crim01 = crim01[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.crim01, k = 1)
library(class)
library(MASS)
set.seed(1)
knn.pred = knn(train.X, test.X, train.crim01, k = 1)
table(knn.pred, crim01.test)
# ii. First subset of predictors: indus, nox, age, dis, rad, tax, ptratio, and lstat
glm.fit = glm(crim01 ~ indus + nox + age + dis+
rad + tax + ptratio +lstat,
family = binomial, data = Boston, subset = train)
glm.probs = predict(glm.fit, Boston.test, type = "response")
glm.pred = rep(0,length(crim01.test))
glm.pred[glm.probs > 0.5] = 1
table(glm.pred, crim01.test)
mean(glm.pred == crim01.test)
lda.fit = lda(crim01 ~ indus + nox + age + dis+
rad + tax + ptratio +lstat,
data = Boston, subset = train)
lda.pred = predict(lda.fit, Boston.test)
lda.class = lda.pred$class
table(lda.class, crim01.test)
mean(lda.class == crim01.test)
train.X = as.matrix(cbind(indus, nox, age, dis,
rad, tax, ptratio, lstat)[train,])
test.X = as.matrix(cbind(indus, nox, age, dis,
rad, tax, ptratio, lstat)[!train,])
train.crim01 = crim01[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.crim01, k = 1)
table(knn.pred, crim01.test)
# iii. Second subset of predictors: indus, nox, age, dis, rad, tax
glm.fit = glm(crim01 ~ indus + nox + age + dis+
rad + tax,
family = binomial, data = Boston, subset = train)
glm.probs = predict(glm.fit, Boston.test, type = "response")
glm.pred = rep(0,length(crim01.test))
glm.pred[glm.probs > 0.5] = 1
table(glm.pred, crim01.test)
mean(glm.pred == crim01.test)
lda.fit = lda(crim01 ~ indus + nox + age + dis+
rad + tax,
data = Boston, subset = train)
lda.pred = predict(lda.fit, Boston.test)
lda.class = lda.pred$class
table(lda.class, crim01.test)
mean(lda.class == crim01.test)
train.X = as.matrix(cbind(indus, nox, age, dis,
rad, tax)[train,])
test.X = as.matrix(cbind(indus, nox, age, dis,
rad, tax)[!train,])
train.crim01 = crim01[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.crim01, k = 1)
table(knn.pred, crim01.test)
# iv. Several subsets of iii in search of the best result, use greedy approach
set.seed(1) #Looking at crim01 there are obvious streaks of 0s and 1s, we sample randomly
# to reduce any impact on our final results
train.ix = as.integer(createDataPartition(Boston$crim01, p = .8, list = FALSE, times = 1))
train = rep(FALSE,length(crim01))
train[train.ix] = TRUE
#train[320:406] = TRUE # second method, just look at last 86 points for test
Boston.test = Boston[!train,]
crim01.test = crim01[!train]
# i. All predictors
glm.fit = glm(crim01 ~ zn + indus + chas + nox + rm + age + dis+
rad + tax + ptratio + black +lstat + medv,
family = binomial, data = Boston, subset = train)
glm.probs = predict(glm.fit, Boston.test, type = "response")
glm.pred = rep(0,length(crim01.test))
glm.pred[glm.probs > 0.5] = 1
table(glm.pred, crim01.test)
mean(glm.pred == crim01.test)
lda.fit = lda(crim01 ~ zn + indus + chas + nox + rm + age + dis+
rad + tax + ptratio + black +lstat + medv,
data = Boston, subset = train)
lda.pred = predict(lda.fit, Boston.test)
lda.class = lda.pred$class
table(lda.class, crim01.test)
mean(lda.class == crim01.test)
train.X = as.matrix(cbind(zn, indus, chas, nox, rm, age, dis,
rad, tax, ptratio, black, lstat, medv)[train,])
test.X = as.matrix(cbind(zn, indus, chas, nox, rm, age, dis,
rad, tax, ptratio, black, lstat, medv)[!train,])
train.crim01 = crim01[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.crim01, k = 1)
table(knn.pred, crim01.test)
# ii. First subset of predictors: indus, nox, age, dis, rad, tax, ptratio, and lstat
glm.fit = glm(crim01 ~ indus + nox + age + dis+
rad + tax + ptratio +lstat,
family = binomial, data = Boston, subset = train)
glm.probs = predict(glm.fit, Boston.test, type = "response")
glm.pred = rep(0,length(crim01.test))
glm.pred[glm.probs > 0.5] = 1
table(glm.pred, crim01.test)
mean(glm.pred == crim01.test)
lda.fit = lda(crim01 ~ indus + nox + age + dis+
rad + tax + ptratio +lstat,
data = Boston, subset = train)
lda.pred = predict(lda.fit, Boston.test)
lda.class = lda.pred$class
table(lda.class, crim01.test)
mean(lda.class == crim01.test)
train.X = as.matrix(cbind(indus, nox, age, dis,
rad, tax, ptratio, lstat)[train,])
test.X = as.matrix(cbind(indus, nox, age, dis,
rad, tax, ptratio, lstat)[!train,])
train.crim01 = crim01[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.crim01, k = 1)
table(knn.pred, crim01.test)
# iii. Second subset of predictors: indus, nox, age, dis, rad, tax
glm.fit = glm(crim01 ~ indus + nox + age + dis+
rad + tax,
family = binomial, data = Boston, subset = train)
glm.probs = predict(glm.fit, Boston.test, type = "response")
glm.pred = rep(0,length(crim01.test))
glm.pred[glm.probs > 0.5] = 1
table(glm.pred, crim01.test)
mean(glm.pred == crim01.test)
lda.fit = lda(crim01 ~ indus + nox + age + dis+
rad + tax,
data = Boston, subset = train)
lda.pred = predict(lda.fit, Boston.test)
lda.class = lda.pred$class
table(lda.class, crim01.test)
mean(lda.class == crim01.test)
